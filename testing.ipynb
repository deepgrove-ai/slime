{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f97ced9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1725454e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d7d0dfb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading veomni_first_sample.pt...\n",
      "Data keys: dict_keys(['tokens', 'log_probs'])\n",
      "Tokens length: 8391\n",
      "Log probs length: 8390\n",
      "Loading Qwen/Qwen3-0.6B model and tokenizer...\n",
      "Input ids shape: torch.Size([1, 8391])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2011871/2773944260.py:32: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input_ids = torch.tensor(tokens).reshape(1, -1).to(model.device, dtype=torch.int64)\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading veomni_first_sample.pt...\")\n",
    "data = torch.load(\"veomni_first_sample.pt\")\n",
    "\n",
    "print(f\"Data keys: {data.keys()}\")\n",
    "print(f\"Tokens length: {len(data['tokens'])}\")\n",
    "print(f\"Log probs length: {len(data['log_probs'])}\")\n",
    "\n",
    "tokens = data[\"tokens\"]\n",
    "reference_log_probs = data[\"log_probs\"]\n",
    "\n",
    "# Convert tensors to lists if needed\n",
    "# if isinstance(tokens, torch.Tensor):\n",
    "#     tokens = tokens.tolist()\n",
    "# if isinstance(reference_log_probs, torch.Tensor):\n",
    "#     reference_log_probs = reference_log_probs.tolist()\n",
    "\n",
    "# print(f\"Tokens type: {type(tokens[0])}\")\n",
    "# print(f\"First few tokens: {tokens[:10]}\")\n",
    "\n",
    "# Load Qwen model and tokenizer\n",
    "print(\"Loading Qwen/Qwen3-0.6B model and tokenizer...\")\n",
    "model_name = \"Qwen/Qwen3-0.6B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, attn_implementation=\"flash_attention_2\", dtype=\"bfloat16\", device_map=\"auto\")\n",
    "\n",
    "# Convert tokens to text\n",
    "# text = tokenizer.decode(tokens)\n",
    "# print(f\"Decoded text length: {len(text)} characters\")\n",
    "\n",
    "# Re-tokenize to get proper tokenization\n",
    "# tokenized = tokenizer(text, return_tensors=\"pt\")\n",
    "input_ids = torch.tensor(tokens).reshape(1, -1).to(model.device, dtype=torch.int64)\n",
    "print(f\"Input ids shape: {input_ids.shape}\")\n",
    "# Get logits from model\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids)\n",
    "    logits = outputs.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "64ded35b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log probs shape: torch.Size([1, 8391, 151936])\n"
     ]
    }
   ],
   "source": [
    "log_probs = torch.log_softmax(logits, dim=-1)\n",
    "print(f\"Log probs shape: {log_probs.shape}\")\n",
    "\n",
    "# Extract log probabilities for the actual tokens (excluding the last token)\n",
    "# We need log_probs[i] for token[i+1]\n",
    "model_log_probs = []\n",
    "for i in range(len(tokens) - 1):\n",
    "    token_id = tokens[i + 1]\n",
    "    log_prob = log_probs[0, i, token_id].item()\n",
    "    model_log_probs.append(log_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ca731c0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2011871/1212021861.py:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  model_log_probs = torch.tensor(model_log_probs, device=torch.device(\"cuda\")).bfloat16()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([-1.0750e+01, -4.4922e-02, -7.7188e+00,  ..., -2.1935e-05,\n",
       "        -1.1921e-06, -8.9111e-03], device='cuda:0', dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_log_probs = torch.tensor(model_log_probs, device=torch.device(\"cuda\")).bfloat16()\n",
    "model_log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "34ecd7eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0004, device='cuda:0', dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# l2 norm between model_log_probs and reference_log_probs\n",
    "\n",
    "torch.norm(model_log_probs - reference_log_probs) / len(model_log_probs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee2f313",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f6028482",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = torch.load(\"./test/debug_rollout_data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6d7f2689",
   "metadata": {},
   "outputs": [],
   "source": [
    "megatron_data = torch.load(open(\"./megatron_rollout_data_0.pt\", \"rb\"))\n",
    "veomni_data = torch.load(open(\"./veomni_padded_batches_0.pt\", \"rb\"))\n",
    "fsdp_data = torch.load(open(\"./fsdp_padded_batches_0.pt\", \"rb\"))\n",
    "\n",
    "# megatron_data[\"abs_advantages\"] = [k.abs() for k in megatron_data[\"advantages\"]]\n",
    "# megatron_data[\"abs_advantages\"] = [k.abs() for k in megatron_data[\"advantages\"]]\n",
    "# print(megatron_data, veomni_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311d807f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tokens': tensor([[151644,    872,    198,  ...,  13038,     25,  48297]],\n",
      "       device='cuda:0'), 'loss_masks': tensor([[0, 0, 0,  ..., 1, 1, 1]], device='cuda:0', dtype=torch.int32), 'rewards': tensor([0.], device='cuda:0'), 'raw_reward': [0], 'log_probs': tensor([[-1.0750e+01, -4.4922e-02, -7.7188e+00,  ..., -2.0266e-05,\n",
      "         -1.0729e-06, -8.9111e-03]], device='cuda:0', dtype=torch.bfloat16), 'advantages': tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0'), 'returns': tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0'), 'abs_advantages': tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')}\n",
      "8391\n",
      "8390\n",
      "8390\n",
      "tensor(8192, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# len(veomni_data)\n",
    "print(veomni_data[0])\n",
    "print(len(veomni_data[0][\"tokens\"][0]))\n",
    "print(len(veomni_data[0][\"log_probs\"][0]))\n",
    "print(len(fsdp_data[0][\"log_probs\"][0]))\n",
    "print(veomni_data[0][\"loss_masks\"].sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d518ca86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "17e9adc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: torch.Size([8391])\n",
      "tensor([ 4279,     8,   323,   220,    19,    23,   320,  1291,  4279,   568,\n",
      "         2806,  6144,    13,  2055,  8318,   382, 15666, 13038,    25, 48297],\n",
      "       device='cuda:0', dtype=torch.int32)\n",
      "{'tokens': tensor([151644,    872,    198,  ...,  13038,     25,  48297], device='cuda:0',\n",
      "       dtype=torch.int32), 'log_probs': tensor([-1.0750e+01, -4.4922e-02, -7.7188e+00,  ..., -2.0266e-05,\n",
      "        -1.0729e-06, -8.9111e-03], device='cuda:0', dtype=torch.bfloat16), 'response_lengths': 8192}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2011871/994799816.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  megatron_first_sample_tokens = torch.tensor(megatron_first_sample_tokens, device=torch.device(\"cuda\")).int()\n",
      "/tmp/ipykernel_2011871/994799816.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  veomni_first_sample_tokens = torch.tensor(veomni_first_sample_tokens, device=torch.device(\"cuda\")).int()\n",
      "/tmp/ipykernel_2011871/994799816.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  assert not torch.allclose(torch.tensor(second_samples_tokens, device=torch.device(\"cuda\")), torch.tensor(megatron_first_sample_tokens, device=torch.device(\"cuda\")))\n"
     ]
    }
   ],
   "source": [
    "megatron_first_sample_tokens = megatron_data[\"tokens\"][0]\n",
    "veomni_first_sample_tokens = veomni_data[0][\"tokens\"][0]\n",
    "first_samples_tokens = samples['samples'][0]['tokens']\n",
    "second_samples_tokens = samples['samples'][1]['tokens']\n",
    "\n",
    "# Convert all to int32 cuda tensors\n",
    "megatron_first_sample_tokens = torch.tensor(megatron_first_sample_tokens, device=torch.device(\"cuda\")).int()\n",
    "veomni_first_sample_tokens = torch.tensor(veomni_first_sample_tokens, device=torch.device(\"cuda\")).int()\n",
    "first_samples_tokens = torch.tensor(first_samples_tokens, device=torch.device(\"cuda\")).int()\n",
    "second_samples_tokens = torch.tensor(second_samples_tokens, device=torch.device(\"cuda\")).int()\n",
    "\n",
    "# Assert shpaes are equal for first samples\n",
    "assert megatron_first_sample_tokens.shape == veomni_first_sample_tokens.shape == first_samples_tokens.shape, f\"{megatron_first_sample_tokens.shape} != {veomni_first_sample_tokens.shape} != {first_samples_tokens.shape}\"\n",
    "\n",
    "print(f\"shape: {megatron_first_sample_tokens.shape}\")\n",
    "assert torch.allclose(megatron_first_sample_tokens, veomni_first_sample_tokens)\n",
    "assert torch.allclose(first_samples_tokens, megatron_first_sample_tokens)\n",
    "assert not torch.allclose(torch.tensor(second_samples_tokens, device=torch.device(\"cuda\")), torch.tensor(megatron_first_sample_tokens, device=torch.device(\"cuda\")))\n",
    "# print(veomni_first_sample_tokens[0:20])\n",
    "\n",
    "print(megatron_first_sample_tokens[-20:])\n",
    "data = {\n",
    "  'tokens': veomni_first_sample_tokens,\n",
    "  'log_probs': veomni_data[0][\"log_probs\"][0],\n",
    "  'response_lengths': len(megatron_data[\"log_probs\"][0])\n",
    "}\n",
    "print(data)\n",
    "\n",
    "torch.save(data, \"veomni_first_sample.pt\")\n",
    "# megatron_first_sample_tokens[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e6423302",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8192\n",
      "8390\n",
      "8192\n",
      "8192\n",
      "tensor([-1.2054e-03,  0.0000e+00, -7.6599e-03,  ..., -2.0266e-05,\n",
      "        -1.0729e-06, -8.9111e-03], device='cuda:0', dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2011871/1820269240.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  megatron_lprobs = torch.tensor(megatron_lprobs, device=torch.device(\"cuda\")).bfloat16()\n",
      "/tmp/ipykernel_2011871/1820269240.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  veomni_lprobs = torch.tensor(veomni_lprobs, device=torch.device(\"cuda\")).bfloat16()\n",
      "/tmp/ipykernel_2011871/1820269240.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  fsdp_lprobs = torch.tensor(fsdp_lprobs, device=torch.device(\"cuda\")).bfloat16()\n"
     ]
    }
   ],
   "source": [
    "megatron_lprobs = megatron_data[\"log_probs\"][0]\n",
    "veomni_lprobs = veomni_data[0][\"log_probs\"][0]\n",
    "fsdp_lprobs = fsdp_data[0][\"log_probs\"][0]\n",
    "sample_lprobs = samples['samples'][0]['rollout_log_probs']\n",
    "\n",
    "# megatron_lprobs\n",
    "# Convert all to bf16 cuda tensors  \n",
    "megatron_lprobs = torch.tensor(megatron_lprobs, device=torch.device(\"cuda\")).bfloat16()\n",
    "veomni_lprobs = torch.tensor(veomni_lprobs, device=torch.device(\"cuda\")).bfloat16()\n",
    "sample_lprobs = torch.tensor(sample_lprobs, device=torch.device(\"cuda\")).bfloat16()\n",
    "fsdp_lprobs = torch.tensor(fsdp_lprobs, device=torch.device(\"cuda\")).bfloat16()\n",
    "# Print lengths\n",
    "print(len(megatron_lprobs))\n",
    "print(len(veomni_lprobs))\n",
    "print(len(sample_lprobs))\n",
    "veomni_lprobs = veomni_lprobs[-len(sample_lprobs):]\n",
    "fsdp_lprobs = fsdp_lprobs[-len(sample_lprobs):]\n",
    "print(len(veomni_lprobs))\n",
    "print(veomni_lprobs)\n",
    "# assert torch.allclose(megatron_lprobs, veomni_lprobs)\n",
    "# assert torch.allclose(sample_lprobs, megatron_lprobs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86d580c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-3.0994e-05, -5.4598e-05, -1.1921e-06, -0.0000e+00, -3.0975e-03],\n",
      "       device='cuda:0', dtype=torch.bfloat16)\n",
      "tensor([-2.5177e-04, -4.8637e-04, -2.0266e-05, -1.0729e-06, -8.9111e-03],\n",
      "       device='cuda:0', dtype=torch.bfloat16)\n",
      "tensor([-2.8419e-04, -4.2152e-04, -2.4676e-05, -1.1921e-06, -1.0071e-02],\n",
      "       device='cuda:0', dtype=torch.bfloat16)\n",
      "tensor([-2.3484e-05, -5.4598e-05, -8.3447e-07,  0.0000e+00, -3.6163e-03],\n",
      "       device='cuda:0', dtype=torch.bfloat16)\n"
     ]
    }
   ],
   "source": [
    "print(megatron_lprobs[:5])\n",
    "print(veomni_lprobs[:5])\n",
    "print(fsdp_lprobs[:5])\n",
    "print(sample_lprobs[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f0096a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.allclose(megatron_lprobs, sample_lprobs, atol=1e-1))\n",
    "print(torch.allclose(veomni_lprobs, sample_lprobs, atol=1e-1))\n",
    "print(torch.allclose(megatron_lprobs, veomni_lprobs, atol=1e-1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac84a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print l2 norm of the difference between megatron_lprobs and veomni_lprobs\n",
    "print(torch.norm(megatron_lprobs - veomni_lprobs)/len(megatron_lprobs))\n",
    "print(torch.norm(megatron_lprobs - sample_lprobs)/len(megatron_lprobs))\n",
    "print(torch.norm(veomni_lprobs - sample_lprobs)/len(veomni_lprobs))\n",
    "print(torch.norm(fsdp_lprobs - veomni_lprobs)/len(fsdp_lprobs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b8d521",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print max of the difference between megatron_lprobs and veomni_lprobs\n",
    "print(torch.max(torch.abs(megatron_lprobs - veomni_lprobs)))\n",
    "print(torch.max(torch.abs(megatron_lprobs - sample_lprobs)))\n",
    "print(torch.max(torch.abs(veomni_lprobs - sample_lprobs)))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
