{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2850efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "# print(os.environ['LD_PRELOAD'])\n",
    "# os.environ['LD_PRELOAD'] = '/home/ubuntu/slime/working/.venv/lib/python3.10/site-packages/torch_memory_saver_hook_mode_preload.abi3.so'\n",
    "# os.environ[\"TMS_INIT_ENABLE\"] = \"\"\n",
    "# os.environ[\"TMS_INIT_ENABLE_CPU_BACKUP\"] = \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec68889a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_memory_saver import configure_subprocess, torch_memory_saver\n",
    "with configure_subprocess():\n",
    "  torch_memory_saver._ensure_initialized()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f97ced9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "0\n",
      "NVIDIA A100-SXM4-80GB\n",
      "GPU 0 memory: 0.75 GB (before)\n",
      "806879232\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "def get_and_print_gpu_memory(message, gpu_id=0):\n",
    "    \"\"\"Print GPU memory usage with optional message\"\"\"\n",
    "    mem = torch.cuda.device_memory_used(gpu_id)\n",
    "    print(f\"GPU {gpu_id} memory: {mem / 1024 ** 3:.2f} GB ({message})\")\n",
    "    return mem\n",
    "\n",
    "# Print devices\n",
    "print(torch.cuda.device_count())\n",
    "print(torch.cuda.current_device())\n",
    "print(torch.cuda.get_device_name(torch.cuda.current_device()))\n",
    "print(get_and_print_gpu_memory(\"before\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0d988fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 0 memory: 5.89 GB (after_alloc)\n",
      "GPU 0 memory: 1.24 GB (after_pause)\n",
      "GPU 0 memory: 5.89 GB (after_resume)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6328221696"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch_memory_saver import torch_memory_saver\n",
    "torch_memory_saver.hook_mode = \"torch\"\n",
    "import torch\n",
    "with torch_memory_saver.region():\n",
    "  tensor1 = torch.full((5_000_000_000,), 100, dtype=torch.uint8, device='cuda')\n",
    "get_and_print_gpu_memory(\"after_alloc\")\n",
    "torch_memory_saver.pause()\n",
    "\n",
    "get_and_print_gpu_memory(\"after_pause\")\n",
    "torch_memory_saver.resume()\n",
    "get_and_print_gpu_memory(\"after_resume\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1725454e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "172b01a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from slime.utils.memory_utils import available_memory\n",
    "available_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d0dfb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Loading veomni_first_sample.pt...\")\n",
    "# data = torch.load(\"veomni_first_sample.pt\")\n",
    "\n",
    "# print(f\"Data keys: {data.keys()}\")\n",
    "# print(f\"Tokens length: {len(data['tokens'])}\")\n",
    "# print(f\"Log probs length: {len(data['log_probs'])}\")\n",
    "\n",
    "# tokens = data[\"tokens\"]\n",
    "# reference_log_probs = data[\"log_probs\"]\n",
    "\n",
    "# Convert tensors to lists if needed\n",
    "# if isinstance(tokens, torch.Tensor):\n",
    "#     tokens = tokens.tolist()\n",
    "# if isinstance(reference_log_probs, torch.Tensor):\n",
    "#     reference_log_probs = reference_log_probs.tolist()\n",
    "\n",
    "# print(f\"Tokens type: {type(tokens[0])}\")\n",
    "# print(f\"First few tokens: {tokens[:10]}\")\n",
    "\n",
    "# Load Qwen model and tokenizer\n",
    "print(\"Loading Qwen/Qwen3-0.6B model and tokenizer...\")\n",
    "# model_name = \"Qwen/Qwen3-0.6B\"\n",
    "model_name = \"Qwen/Qwen3-4B\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_name, attn_implementation=\"flash_attention_2\", dtype=\"bfloat16\", device_map=\"auto\")\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name,  dtype=\"bfloat16\", device_map=\"auto\")\n",
    "\n",
    "# Convert tokens to text\n",
    "# text = tokenizer.decode(tokens)\n",
    "# print(f\"Decoded text length: {len(text)} characters\")\n",
    "\n",
    "# Re-tokenize to get proper tokenization\n",
    "# tokenized = tokenizer(text, return_tensors=\"pt\")\n",
    "# input_ids = torch.tensor(tokens).reshape(1, -1).to(model.device, dtype=torch.int64)\n",
    "# print(f\"Input ids shape: {input_ids.shape}\")\n",
    "# # Get logits from model\n",
    "# with torch.no_grad():\n",
    "#     outputs = model(input_ids)\n",
    "#     logits = outputs.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab09ca44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_memory_saver import torch_memory_saver\n",
    "from slime.utils.memory_utils import clear_memory\n",
    "\n",
    "print(\"Pausing memory saver\")\n",
    "print(available_memory())\n",
    "torch_memory_saver.pause()\n",
    "clear_memory()\n",
    "print(\"After pause\")\n",
    "print(available_memory())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c743a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_memory_saver import torch_memory_saver\n",
    "\n",
    "print(\"resuming memory saver\")\n",
    "print(available_memory())\n",
    "torch_memory_saver.resume()\n",
    "print(\"After resume\")\n",
    "print(available_memory())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bebb47ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from slime.utils.memory_utils import clear_memory\n",
    "# del model\n",
    "clear_memory()\n",
    "print(available_memory())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ded35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_probs = torch.log_softmax(logits, dim=-1)\n",
    "print(f\"Log probs shape: {log_probs.shape}\")\n",
    "\n",
    "# Extract log probabilities for the actual tokens (excluding the last token)\n",
    "# We need log_probs[i] for token[i+1]\n",
    "model_log_probs = []\n",
    "for i in range(len(tokens) - 1):\n",
    "    token_id = tokens[i + 1]\n",
    "    log_prob = log_probs[0, i, token_id].item()\n",
    "    model_log_probs.append(log_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca731c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_log_probs = torch.tensor(model_log_probs, device=torch.device(\"cuda\")).bfloat16()\n",
    "model_log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ecd7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# l2 norm between model_log_probs and reference_log_probs\n",
    "\n",
    "torch.norm(model_log_probs - reference_log_probs) / len(model_log_probs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee2f313",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6028482",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = torch.load(\"./test/debug_rollout_data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7f2689",
   "metadata": {},
   "outputs": [],
   "source": [
    "megatron_data = torch.load(open(\"./megatron_rollout_data_0.pt\", \"rb\"))\n",
    "veomni_data = torch.load(open(\"./veomni_padded_batches_0.pt\", \"rb\"))\n",
    "fsdp_data = torch.load(open(\"./fsdp_padded_batches_0.pt\", \"rb\"))\n",
    "\n",
    "# megatron_data[\"abs_advantages\"] = [k.abs() for k in megatron_data[\"advantages\"]]\n",
    "# megatron_data[\"abs_advantages\"] = [k.abs() for k in megatron_data[\"advantages\"]]\n",
    "# print(megatron_data, veomni_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311d807f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(veomni_data)\n",
    "print(veomni_data[0])\n",
    "print(len(veomni_data[0][\"tokens\"][0]))\n",
    "print(len(veomni_data[0][\"log_probs\"][0]))\n",
    "print(len(fsdp_data[0][\"log_probs\"][0]))\n",
    "print(veomni_data[0][\"loss_masks\"].sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d518ca86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e9adc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "megatron_first_sample_tokens = megatron_data[\"tokens\"][0]\n",
    "veomni_first_sample_tokens = veomni_data[0][\"tokens\"][0]\n",
    "first_samples_tokens = samples['samples'][0]['tokens']\n",
    "second_samples_tokens = samples['samples'][1]['tokens']\n",
    "\n",
    "# Convert all to int32 cuda tensors\n",
    "megatron_first_sample_tokens = torch.tensor(megatron_first_sample_tokens, device=torch.device(\"cuda\")).int()\n",
    "veomni_first_sample_tokens = torch.tensor(veomni_first_sample_tokens, device=torch.device(\"cuda\")).int()\n",
    "first_samples_tokens = torch.tensor(first_samples_tokens, device=torch.device(\"cuda\")).int()\n",
    "second_samples_tokens = torch.tensor(second_samples_tokens, device=torch.device(\"cuda\")).int()\n",
    "\n",
    "# Assert shpaes are equal for first samples\n",
    "assert megatron_first_sample_tokens.shape == veomni_first_sample_tokens.shape == first_samples_tokens.shape, f\"{megatron_first_sample_tokens.shape} != {veomni_first_sample_tokens.shape} != {first_samples_tokens.shape}\"\n",
    "\n",
    "print(f\"shape: {megatron_first_sample_tokens.shape}\")\n",
    "assert torch.allclose(megatron_first_sample_tokens, veomni_first_sample_tokens)\n",
    "assert torch.allclose(first_samples_tokens, megatron_first_sample_tokens)\n",
    "assert not torch.allclose(torch.tensor(second_samples_tokens, device=torch.device(\"cuda\")), torch.tensor(megatron_first_sample_tokens, device=torch.device(\"cuda\")))\n",
    "# print(veomni_first_sample_tokens[0:20])\n",
    "\n",
    "print(megatron_first_sample_tokens[-20:])\n",
    "data = {\n",
    "  'tokens': veomni_first_sample_tokens,\n",
    "  'log_probs': veomni_data[0][\"log_probs\"][0],\n",
    "  'response_lengths': len(megatron_data[\"log_probs\"][0])\n",
    "}\n",
    "print(data)\n",
    "\n",
    "torch.save(data, \"veomni_first_sample.pt\")\n",
    "# megatron_first_sample_tokens[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6423302",
   "metadata": {},
   "outputs": [],
   "source": [
    "megatron_lprobs = megatron_data[\"log_probs\"][0]\n",
    "veomni_lprobs = veomni_data[0][\"log_probs\"][0]\n",
    "fsdp_lprobs = fsdp_data[0][\"log_probs\"][0]\n",
    "sample_lprobs = samples['samples'][0]['rollout_log_probs']\n",
    "\n",
    "# megatron_lprobs\n",
    "# Convert all to bf16 cuda tensors  \n",
    "megatron_lprobs = torch.tensor(megatron_lprobs, device=torch.device(\"cuda\")).bfloat16()\n",
    "veomni_lprobs = torch.tensor(veomni_lprobs, device=torch.device(\"cuda\")).bfloat16()\n",
    "sample_lprobs = torch.tensor(sample_lprobs, device=torch.device(\"cuda\")).bfloat16()\n",
    "fsdp_lprobs = torch.tensor(fsdp_lprobs, device=torch.device(\"cuda\")).bfloat16()\n",
    "# Print lengths\n",
    "print(len(megatron_lprobs))\n",
    "print(len(veomni_lprobs))\n",
    "print(len(sample_lprobs))\n",
    "veomni_lprobs = veomni_lprobs[-len(sample_lprobs):]\n",
    "fsdp_lprobs = fsdp_lprobs[-len(sample_lprobs):]\n",
    "print(len(veomni_lprobs))\n",
    "print(veomni_lprobs)\n",
    "# assert torch.allclose(megatron_lprobs, veomni_lprobs)\n",
    "# assert torch.allclose(sample_lprobs, megatron_lprobs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86d580c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(megatron_lprobs[:5])\n",
    "print(veomni_lprobs[:5])\n",
    "print(fsdp_lprobs[:5])\n",
    "print(sample_lprobs[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f0096a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.allclose(megatron_lprobs, sample_lprobs, atol=1e-1))\n",
    "print(torch.allclose(veomni_lprobs, sample_lprobs, atol=1e-1))\n",
    "print(torch.allclose(megatron_lprobs, veomni_lprobs, atol=1e-1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac84a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print l2 norm of the difference between megatron_lprobs and veomni_lprobs\n",
    "print(torch.norm(megatron_lprobs - veomni_lprobs)/len(megatron_lprobs))\n",
    "print(torch.norm(megatron_lprobs - sample_lprobs)/len(megatron_lprobs))\n",
    "print(torch.norm(veomni_lprobs - sample_lprobs)/len(veomni_lprobs))\n",
    "print(torch.norm(fsdp_lprobs - veomni_lprobs)/len(fsdp_lprobs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b8d521",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print max of the difference between megatron_lprobs and veomni_lprobs\n",
    "print(torch.max(torch.abs(megatron_lprobs - veomni_lprobs)))\n",
    "print(torch.max(torch.abs(megatron_lprobs - sample_lprobs)))\n",
    "print(torch.max(torch.abs(veomni_lprobs - sample_lprobs)))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
